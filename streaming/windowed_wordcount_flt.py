#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

"""
 Counts words in text encoded with UTF8 received from the network every second using a 
 defined time window and interval.

 Usage: recoverable_network_wordcount.py <hostname> <port> <checkpoint-directory> <window-length>
                                         <window-interval> <regular-expression> 

   <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive
   data. <checkpoint-directory> directory to HDFS-compatible file system which checkpoint data
   <window-length> is the lenght of the window that the streaming msg are processed
   <window-int> is the interval that the window is refreshed
   <regular-expression> is a python compatible regex used to filter the msgs
 

 To run this on your local machine, you need to first run a Netcat server
    `$ nc -lk 9999`
 To generate some more intense trafic you can use the /var/log/syslog by running 
    `$ tail -f /var/log/syslog | nc -lk 9999`

 and then run the example
    `$ bin/spark-submit --master local[2] examples/src/main/python/streaming/recoverable_network_wordcount.py \
        localhost 9999 ~/checkpoint/ 30 2 '[A-Z]+'`
 
 If the directory ~/checkpoint/ does not exist (e.g. running for the first time), it will create
 a new StreamingContext (will print "Creating new context" to the console). Otherwise, if
 checkpoint data exists in ~/checkpoint/, then it will create StreamingContext from
 the checkpoint data.
"""
from __future__ import print_function

import os
import sys
import re

from pyspark import SparkContext
from pyspark.streaming import StreamingContext


#Define clear screen function
def cls():
    tmp = os.system('clear')


#Define a funciton that returns true when the regular match of the regular expression
# pattern matches the full string
def reFullMatch(string,pattern):
    tmp = re.match(pattern,string)
    if tmp:
        if tmp.group(0) == string:
            return True
        else:
            return False
    else:
        return False


def createContext(host, port, checkpointDirectory, wd_length, wd_int, regexp ):
    # If you do not see this printed, that means the StreamingContext has been loaded
    # from the new checkpoint
    print("Creating new context! Working in the current path: %s !" % os.getcwd())
    sc = SparkContext(appName="PythonStreamingWindowedWC_Cluster")
    sc.setLogLevel('ERROR')
    ssc = StreamingContext(sc, 1)

    # Create a socket stream on target ip:port and count the
    # words in input stream of \n delimited text (eg. generated by 'nc')

    #To generate a nice stream we can use the syslog by 
    # runing tail -f /var/log/syslog | nc host port 
    lines = ssc.socketTextStream(host, port)
    words = lines.flatMap(lambda line: line.split(" "))
    #Filter words following the regular expression
    wordsFlt = words.filter(lambda words: reFullMatch(words, regexp ))
    pairs = wordsFlt.map(lambda x: (x, 1))
    wordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y,
                                            lambda x, y: x - y,
                                            wd_length, wd_int)

    def echo(time, rdd):
        #This function is executed for each recieved RDD
        # and is used to update the displayed results
 
        # Result are sorted by word count, largest on top.
        rddStr= rdd.sortBy(lambda wordTuple: wordTuple[1],ascending=False)
        n_elem = rddStr.count()
        lst = rddStr.collect()
        #Terminal screen cleaned
        cls()
        print("------------------------------------------------")
        print("Time: %s. Processed words: %s" % (time,n_elem))
        print("------------------------------------------------")
        if n_elem > 25:
            for elem in lst[:25]:
                print("%s=> %s"%(elem[0],elem[1]))
        elif n_elem > 0:
            for elem in lst:
                print("%s=> %s"%(elem[0],elem[1]))

    
    wordCounts.foreachRDD(echo)
    ssc.checkpoint(checkpointDirectory)
    return ssc

if __name__ == "__main__":
    if len(sys.argv) != 7:
        print("Usage: recoverable_network_wordcount.py <hostname> <port> "
              "<checkpoint-directory> <window length [s]> <window interval [s]> " 
"<regular expression>", file=sys.stderr)
        exit(-1)
    host, port, checkpoint, wd_len, wd_int, regexp  = sys.argv[1:]
    ssc = StreamingContext.getOrCreate(checkpoint,
                                       lambda: createContext(host, 
                                                             int(port), 
                                                             checkpoint,
                                                             int(wd_len),
                                                             int(wd_int),
                                                             regexp))
    ssc.start()
    ssc.awaitTermination()
